---
globs: **/sync/**
alwaysApply: false
---
# Airweave Sync Architecture - Deep Dive

## Overview

The sync module orchestrates data flow from sources to destinations using a highly concurrent, pull-based asynchronous architecture with sophisticated backpressure control, real-time progress tracking, and automatic OAuth token management.

## Core Architecture Principles

### 1. Pull-Based Concurrency Model
- **Worker Pool Pattern**: Uses `AsyncWorkerPool` with semaphore-controlled concurrency (default: 20 workers)
- **Pull vs Push**: Workers pull entities from the stream only when ready, preventing system overload
- **Backpressure**: `AsyncSourceStream` uses bounded queues (default: 10000) to naturally throttle producers

### 2. Separation of Concerns
- **Producer/Consumer Decoupling**: Source generation runs independently from entity processing
- **Modular Pipeline**: Each stage (enrich, transform, vectorize, persist) is isolated
- **Resource Isolation**: Database sessions created only when needed to minimize connection usage

## Component Deep Dive

### SyncFactory
**Purpose**: Factory pattern for creating fully-initialized sync orchestrators

**Key Responsibilities**:
- Creates `SyncContext` with all required dependencies
- Initializes transformer cache to eliminate runtime database lookups
- Configures contextual logging with sync metadata
- Manages OAuth token initialization

**Critical Implementation Details**:
```python
# CRITICAL FIX: Pre-loads transformers to avoid 1.5s database lookups per entity
await sync_context.router.initialize_transformer_cache(db)
```

### SyncContext
**Purpose**: Immutable container holding all resources needed during sync execution

**Components**:
- `source`: Source instance with OAuth token management
- `destinations`: List of destination instances (vector/graph databases)
- `embedding_model`: Model for text vectorization
- `transformers`: Dictionary of callable transformers
- `sync`, `sync_job`, `dag`: Configuration objects
- `progress`: Real-time progress tracker with Redis pubsub
- `router`: DAG-based entity routing engine
- `entity_map`: Maps entity types to UUIDs
- `logger`: Contextual logger with sync metadata

**Design Pattern**: Context Object - passes all dependencies as a single parameter

### SyncOrchestrator
**Purpose**: Coordinates the entire sync workflow with error handling and progress tracking

**Workflow Stages**:
1. **Start**: Updates job status to IN_PROGRESS
2. **Process**: Manages entity streaming and concurrent processing
3. **Complete/Fail**: Updates final status with statistics

**Key Methods**:
- `run()`: Main entry point with try/catch for proper cleanup
- `_process_entities()`: Implements pull-based processing loop
- `_handle_completed_tasks()`: Cleans completed tasks and checks for errors
- `_wait_for_remaining_tasks()`: Ensures all tasks complete before finishing

**Concurrency Management**:
```python
# Workers pull entities only when ready
async for entity in stream.get_entities():
    if entity.airweave_system_metadata.should_skip:
        # Skip without using a worker
        await sync_context.progress.increment("skipped")
        continue

    # Submit to worker pool (blocks if all workers busy)
    task = await worker_pool.submit(...)
```

### AsyncSourceStream
**Purpose**: Manages async streaming with backpressure between producer and consumer

**Architecture**:
- **Producer Task**: Runs independently, filling queue from source generator
- **Bounded Queue**: Implements backpressure (blocks producer when full)
- **Consumer Interface**: `get_entities()` yields items as they become available
- **Error Propagation**: Producer exceptions are captured and re-raised to consumer

**Key Features**:
- Context manager support for proper resource cleanup
- Progress logging every 50 items
- Graceful shutdown with timeout handling
- Sentinel value (None) signals end of stream

### EntityProcessor
**Purpose**: Processes entities through a multi-stage pipeline

**Pipeline Stages**:
1. **Duplicate Detection**: Tracks processed entities to prevent reprocessing
2. **Enrichment**: Adds metadata (sync_id, timestamps, organization_id)
3. **Action Determination**: Decides INSERT/UPDATE/DELETE/KEEP based on hash comparison
4. **Transformation**: Routes through DAG-defined transformers
5. **Vectorization**: Computes embeddings for text content
6. **Persistence**: Writes to destination databases

**Performance Optimizations**:
- Detailed timing logs for each stage
- Database sessions created only when needed
- Concurrent vector computation for multiple entities
- Skip early for entities marked with `should_skip`

**Error Handling**:
- Catches exceptions per entity (doesn't fail entire sync)
- Marks failed entities as "skipped" in progress tracking
- Detailed error logging with entity context

### SyncDAGRouter
**Purpose**: Routes entities through transformation pipeline based on DAG structure

**Key Components**:
- **Execution Route**: Pre-computed routing map for O(1) lookups
- **Transformer Cache**: Pre-loaded transformers to avoid database queries
- **Entity Lineage**: Tracks parent-child relationships through transformations

**Routing Logic**:
```python
# Route map: (producer_node_id, entity_type_id) -> consumer_node_id
route_map[(producer, entity_definition_id)] = consumer_node_id

# Special case: routes to destination return None (stops routing)
if all_edges_go_to_destinations:
    route_map[key] = None
```

**Advanced Features**:
- Supports polymorphic entities (one-to-many transformations)
- Handles multi-path routing through DAG
- Optimized for chunk processing (files → chunks)

### AsyncWorkerPool
**Purpose**: Controls concurrent task execution with semaphore-based limiting

**Implementation Details**:
- **Semaphore Control**: Limits active tasks (prevents resource exhaustion)
- **Task Tracking**: Maintains set of pending tasks with cleanup callbacks
- **Detailed Logging**: Tracks task lifecycle (submit → wait → start → complete)
- **Thread Awareness**: Logs thread IDs for debugging concurrency issues

**Key Methods**:
- `submit()`: Creates task and adds to tracking set
- `_run_with_semaphore()`: Wraps coroutine with semaphore acquisition
- `_handle_task_completion()`: Cleans up completed/failed tasks

### SyncProgress (with PubSub)
**Purpose**: Tracks sync progress with real-time updates via Redis pubsub

**Features**:
- **Atomic Counters**: Thread-safe increment operations with async locks
- **Threshold Publishing**: Publishes updates every N operations (default: 3)
- **Entity Tracking**: Maintains count of unique entities by type
- **Redis Integration**: Publishes to `sync_job:{job_id}` channels

**Statistics Tracked**:
- `inserted`, `updated`, `deleted`: Destination operations
- `kept`: Unchanged entities (hash match)
- `skipped`: Failed or filtered entities
- `entities_encountered`: Count by entity type

**Critical Fix**:
```python
# Async lock prevents race conditions from concurrent workers
async with self._lock:
    setattr(self.stats, stat_name, current_value + amount)
```

### TokenManager
**Purpose**: Manages OAuth2 token refresh for long-running syncs

**Key Features**:
- **Automatic Refresh**: Refreshes tokens before expiry (25-minute intervals)
- **Concurrent Refresh Prevention**: Uses async lock to prevent duplicate refreshes
- **Direct Injection**: Supports non-refreshable tokens (skips refresh)

**Refresh Logic**:
```python
# Only refreshes for specific auth types
if auth_type in (AuthType.oauth2_with_refresh, AuthType.oauth2_with_refresh_rotating):
    # Refresh if older than 25 minutes
    if time_since_refresh > REFRESH_INTERVAL_SECONDS:
        await self._refresh_token()
```

### Async Helpers
**Purpose**: Performance utilities for CPU-bound operations

**Key Utilities**:
- **Shared Thread Pool**: Reuses executor for all CPU-bound tasks
- **Async Hashing**: Non-blocking file/content hashing
- **Stable Serialization**: Consistent object serialization for hashing
- **Chunked File Reading**: Memory-efficient file processing

## Data Flow - Detailed

### 1. Initialization Phase
```
SyncFactory.create_orchestrator()
├── Create SyncContext
│   ├── Initialize source with OAuth token
│   ├── Create destinations
│   ├── Setup embedding model
│   └── Build DAG router
├── Initialize transformer cache (CRITICAL)
├── Create EntityProcessor
└── Create AsyncWorkerPool
```

### 2. Streaming Phase
```
AsyncSourceStream (Producer)
├── Runs in separate task
├── Generates entities from source
├── Puts in bounded queue (backpressure)
└── Signals completion with None

SyncOrchestrator (Consumer)
├── Pulls from stream when worker available
├── Skips marked entities without worker
└── Submits to worker pool
```

### 3. Processing Phase (Per Entity)
```
EntityProcessor.process()
├── Check duplicates
├── Enrich with metadata
├── Determine action (query destination)
├── Route through transformers (DAG)
├── Compute vectors (if needed)
└── Persist to destinations
```

### 4. Progress Tracking
```
Every operation increments counter
├── Async lock ensures thread safety
├── Threshold check (every 3 ops)
├── Publish to Redis if threshold met
└── Subscribers receive real-time updates
```

## Orphaned Workflow Self-Destruct

**Problem**: Workflows may execute after their sync/source_connection is deleted (race condition between schedule deletion and queued workflows).

**Solution**: Self-healing workflows that detect orphaned state and clean up automatically.

**Detection Points**:
1. **Early** (in `create_sync_job_activity`): Checks if sync exists before creating job
   - Returns `{"_orphaned": True, "sync_id": sync_id}` if not found
2. **Late** (in `run_sync_activity`): Catches `NotFoundException` during execution
   - Raises `Exception("ORPHANED_SYNC: Source connection record not found")`

Custom exception types don't serialize cleanly, so we use a string marker.

**Self-Destruct Flow**:
```python
if sync_job_dict.get("_orphaned") or "ORPHANED_SYNC" in str(e):
    # Call self_destruct_orphaned_sync_activity
    await workflow.execute_activity(
        self_destruct_orphaned_sync_activity,
        args=[sync_dict["id"], ctx_dict, reason],
        start_to_close_timeout=timedelta(minutes=5),
        retry_policy=RetryPolicy(maximum_attempts=3),
    )
    return  # Exit gracefully without error
```

**Cleanup Actions** (in `self_destruct_orphaned_sync_activity`):
- Deletes all schedule types: `sync-{id}`, `minute-sync-{id}`, `daily-cleanup-{id}`
- Uses existing `temporal_schedule_service.delete_schedule_handle()`
- Logs with INFO level (🧹 indicators), not ERROR
- Idempotent (safe for concurrent workflows)

**Result**: No "Source connection record not found" errors, graceful workflow exits, automatic schedule cleanup.


## Best Practices

### When Extending
1. Maintain pull-based architecture
2. Use async locks for shared state
3. Create database sessions sparingly
4. Log with contextual information
5. Handle errors at entity level


### Common Pitfalls
1. Don't block the event loop
2. Avoid unbounded concurrency
3. Handle token refresh properly
4. Clean up resources in finally blocks
5. Test with large datasets

This architecture enables efficient processing of millions of entities with optimal resource usage, real-time monitoring, and robust error handling.

   - Final statistics are published
   - Resources are cleaned up

This architecture enables efficient processing of large datasets with optimal resource usage and real-time progress reporting.
